{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sensationalspace/colab/blob/main/learn/generation/langchain/handbook/09-langchain-streaming/09-langchain-streaming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pa8ltlhgOyCG"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/09-langchain-streaming/09-langchain-streaming.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/09-langchain-streaming/09-langchain-streaming.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V63IuGXgOyCI"
      },
      "source": [
        "#### [LangChain Handbook](https://pinecone.io/learn/langchain)\n",
        "\n",
        "# Streaming\n",
        "\n",
        "For LLMs, streaming has become an increasingly popular feature. The idea is to rapidly return tokens as an LLM is generating them, rather than waiting for a full response to be created before returning anything.\n",
        "\n",
        "Streaming is actually very easy to implement for simple use-cases, but it can get complicated when we start including things like Agents which have their own logic running which can block our attempts at streaming. Fortunately, we can make it work â€” it just requires a little extra effort.\n",
        "\n",
        "We'll start easy by implementing streaming to the terminal for LLMs, but by the end of the notebook we'll be handling the more complex task of streaming via FastAPI for Agents.\n",
        "\n",
        "First, let's install all of the libraries we'll be using."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ypRUWTcuOyCJ",
        "outputId": "1af12b07-5eac-4b45-dd1e-6c2fc04945c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.2/66.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m428.8/428.8 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU \\\n",
        "    openai==0.28.0 \\\n",
        "    langchain==0.0.301 \\\n",
        "    fastapi==0.103.1 \\\n",
        "    \"uvicorn[standard]\"==0.23.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maQdfeaKOyCK"
      },
      "source": [
        "## LLM Streaming to Stdout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6uicWDHOyCK"
      },
      "source": [
        "The simplest form of streaming is to simply \"print\" the tokens as they're generated. To set this up we need to initialize an LLM (one that supports streaming, not all do) with two specific parameters:\n",
        "\n",
        "* `streaming=True`, to enable streaming\n",
        "* `callbacks=[SomeCallBackHere()]`, where we pass a LangChain callback class (or list containing multiple).\n",
        "\n",
        "The `streaming` parameter is self-explanatory. The `callbacks` parameter and callback classes less so â€” essentially they act as little bits of code that do something as each token from our LLM is generated. As mentioned, the simplest form of streaming is to print the tokens as they're being generated, like with the `StreamingStdOutCallbackHandler`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-0rr2nXbOyCK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") or \"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\"\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "    temperature=0.0,\n",
        "    model_name=\"gpt-3.5-turbo\",\n",
        "    streaming=True,  # ! important\n",
        "    callbacks=[StreamingStdOutCallbackHandler()]  # ! important\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RcLZjE9OyCK"
      },
      "source": [
        "Now if we run the LLM we'll see the response being _streamed_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "z9BzpkhPOyCK",
        "outputId": "54ce60ce-8cd6-4bb7-a8d0-0e674a7fad8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time, in a small village nestled deep within a lush forest, there lived a young girl named Lily. She was known for her vibrant red hair, sparkling green eyes, and a heart full of curiosity. Lily had always been fascinated by the stories her grandmother would tell her about a magical creature called the Moonlight Unicorn.\n",
            "\n",
            "Legend had it that the Moonlight Unicorn was a rare and majestic creature that only appeared during the full moon. Its coat shimmered like silver, and its horn glowed with a soft, ethereal light. The unicorn was said to possess incredible powers, capable of granting wishes and bringing good fortune to those who encountered it.\n",
            "\n",
            "Driven by her desire to see the Moonlight Unicorn, Lily embarked on a journey through the enchanted forest. Armed with her grandmother's stories and a sense of adventure, she ventured deeper into the woods, following the faint whispers of the wind.\n",
            "\n",
            "As she wandered through the forest, Lily encountered various magical creatures. She met a mischievous sprite who led her to a hidden waterfall, where she discovered a family of playful water nymphs. They taught her the secrets of the forest and shared their wisdom about the Moonlight Unicorn.\n",
            "\n",
            "With newfound knowledge, Lily continued her quest, guided by the moon's gentle glow. She encountered talking animals, wise old trees, and even a friendly dragon who offered her protection on her journey. Each encounter brought her closer to her goal, and her heart swelled with excitement.\n",
            "\n",
            "Days turned into weeks, and weeks turned into months, but Lily never lost hope. She faced countless challenges along the way, from treacherous paths to dark and eerie caves. Yet, her determination never wavered, for she knew that the Moonlight Unicorn was waiting for her somewhere in the depths of the forest.\n",
            "\n",
            "One fateful night, as the full moon bathed the forest in its radiant light, Lily stumbled upon a clearing. There, standing before her, was the Moonlight Unicorn. Its silver coat shimmered, and its horn emitted a soft, enchanting glow. Lily's heart skipped a beat as she realized her dream had come true.\n",
            "\n",
            "With trembling hands, she approached the unicorn, her eyes filled with awe and wonder. The unicorn, sensing her pure heart and unwavering spirit, lowered its head and allowed Lily to touch its horn. In that moment, a surge of magic coursed through her veins, filling her with a sense of purpose and fulfillment.\n",
            "\n",
            "The Moonlight Unicorn spoke to Lily in a gentle, melodic voice. It thanked her for her unwavering determination and rewarded her with a single wish. Lily, after much contemplation, wished for the forest to be protected and preserved for generations to come, ensuring that its magic would never fade away.\n",
            "\n",
            "As her wish was granted, the forest came alive with renewed energy. The trees grew taller, the flowers bloomed brighter, and the animals thrived in harmony. Lily's wish had not only protected the forest but also brought prosperity and joy to the village.\n",
            "\n",
            "From that day forward, Lily became the guardian of the enchanted forest, ensuring its magic remained intact. She shared her stories with the villagers, inspiring them to cherish and protect the natural world around them. And every full moon, Lily would return to the clearing, where she would find solace in the presence of the Moonlight Unicorn, forever grateful for the magical journey that had changed her life."
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessageChunk(content=\"Once upon a time, in a small village nestled deep within a lush forest, there lived a young girl named Lily. She was known for her vibrant red hair, sparkling green eyes, and a heart full of curiosity. Lily had always been fascinated by the stories her grandmother would tell her about a magical creature called the Moonlight Unicorn.\\n\\nLegend had it that the Moonlight Unicorn was a rare and majestic creature that only appeared during the full moon. Its coat shimmered like silver, and its horn glowed with a soft, ethereal light. The unicorn was said to possess incredible powers, capable of granting wishes and bringing good fortune to those who encountered it.\\n\\nDriven by her desire to see the Moonlight Unicorn, Lily embarked on a journey through the enchanted forest. Armed with her grandmother's stories and a sense of adventure, she ventured deeper into the woods, following the faint whispers of the wind.\\n\\nAs she wandered through the forest, Lily encountered various magical creatures. She met a mischievous sprite who led her to a hidden waterfall, where she discovered a family of playful water nymphs. They taught her the secrets of the forest and shared their wisdom about the Moonlight Unicorn.\\n\\nWith newfound knowledge, Lily continued her quest, guided by the moon's gentle glow. She encountered talking animals, wise old trees, and even a friendly dragon who offered her protection on her journey. Each encounter brought her closer to her goal, and her heart swelled with excitement.\\n\\nDays turned into weeks, and weeks turned into months, but Lily never lost hope. She faced countless challenges along the way, from treacherous paths to dark and eerie caves. Yet, her determination never wavered, for she knew that the Moonlight Unicorn was waiting for her somewhere in the depths of the forest.\\n\\nOne fateful night, as the full moon bathed the forest in its radiant light, Lily stumbled upon a clearing. There, standing before her, was the Moonlight Unicorn. Its silver coat shimmered, and its horn emitted a soft, enchanting glow. Lily's heart skipped a beat as she realized her dream had come true.\\n\\nWith trembling hands, she approached the unicorn, her eyes filled with awe and wonder. The unicorn, sensing her pure heart and unwavering spirit, lowered its head and allowed Lily to touch its horn. In that moment, a surge of magic coursed through her veins, filling her with a sense of purpose and fulfillment.\\n\\nThe Moonlight Unicorn spoke to Lily in a gentle, melodic voice. It thanked her for her unwavering determination and rewarded her with a single wish. Lily, after much contemplation, wished for the forest to be protected and preserved for generations to come, ensuring that its magic would never fade away.\\n\\nAs her wish was granted, the forest came alive with renewed energy. The trees grew taller, the flowers bloomed brighter, and the animals thrived in harmony. Lily's wish had not only protected the forest but also brought prosperity and joy to the village.\\n\\nFrom that day forward, Lily became the guardian of the enchanted forest, ensuring its magic remained intact. She shared her stories with the villagers, inspiring them to cherish and protect the natural world around them. And every full moon, Lily would return to the clearing, where she would find solace in the presence of the Moonlight Unicorn, forever grateful for the magical journey that had changed her life.\", additional_kwargs={}, example=False)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from langchain.schema import HumanMessage\n",
        "\n",
        "# create messages to be passed to chat LLM\n",
        "messages = [HumanMessage(content=\"tell me a long story\")]\n",
        "\n",
        "llm(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrwlaByQOyCL"
      },
      "source": [
        "That was surprisingly easy, but things begin to get much more complicated as soon as we begin using agents. Let's first initialize an agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ElMTubKfOyCL"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.agents import load_tools, AgentType, initialize_agent\n",
        "\n",
        "# initialize conversational memory\n",
        "memory = ConversationBufferWindowMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    k=5,\n",
        "    return_messages=True,\n",
        "    output_key=\"output\"\n",
        ")\n",
        "\n",
        "# create a single tool to see how it impacts streaming\n",
        "tools = load_tools([\"llm-math\"], llm=llm)\n",
        "\n",
        "# initialize the agent\n",
        "agent = initialize_agent(\n",
        "    agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True,\n",
        "    max_iterations=3,\n",
        "    early_stopping_method=\"generate\",\n",
        "    return_intermediate_steps=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upS3cMyJOyCL"
      },
      "source": [
        "We already added our `StreamingStdOutCallbackHandler` to the agent as we initialized the agent with the same `llm` as we created with that callback. So let's see what we get when running the agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QB5cHAiOyCL"
      },
      "outputs": [],
      "source": [
        "prompt = \"Hello, how are you?\"\n",
        "\n",
        "agent(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90yyOCWNOyCL"
      },
      "source": [
        "Not bad, but we do now have the issue of streaming the _entire_ output from the LLM. Because we're using an agent, the LLM is instructed to output the JSON format we can see here so that the agent logic can handle tool usage, multiple \"thinking\" steps, and so on. For example, if we ask a math question we'll see this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HosXSA6OyCL"
      },
      "outputs": [],
      "source": [
        "agent(\"what is the square root of 71?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "te0ZFA1iOyCM"
      },
      "source": [
        "It's interesting to see during development but we'll want to clean this streaming up a little in any actual use-case. For that we can go with two approaches â€” either we build a custom callback handler, or use a purpose built callback handler from LangChain (as usual, LangChain has something for everything). Let's first try LangChain's purpose-built `FinalStreamingStdOutCallbackHandler`.\n",
        "\n",
        "We will overwrite the existing `callbacks` attribute found here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNKXGOeLOyCM"
      },
      "outputs": [],
      "source": [
        "agent.agent.llm_chain.llm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3Oq0vTnOyCM"
      },
      "source": [
        "With the new callback handler:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnVM-BK4OyCM"
      },
      "outputs": [],
      "source": [
        "from langchain.callbacks.streaming_stdout_final_only import (\n",
        "    FinalStreamingStdOutCallbackHandler,\n",
        ")\n",
        "\n",
        "agent.agent.llm_chain.llm.callbacks = [\n",
        "    FinalStreamingStdOutCallbackHandler(\n",
        "        answer_prefix_tokens=[\"Final\", \"Answer\"]\n",
        "    )\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsR-uY0POyCM"
      },
      "source": [
        "Let's try it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RGshwntOyCM"
      },
      "outputs": [],
      "source": [
        "agent(\"what is the square root of 71?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7lRIjF8OyCM"
      },
      "source": [
        "Not quite there, we should really clean up the `answer_prefix_tokens` argument but it is hard to get right. It's generally easier to use a custom callback handler like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hvnJVGCOyCM"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "class CallbackHandler(StreamingStdOutCallbackHandler):\n",
        "    def __init__(self):\n",
        "        self.content: str = \"\"\n",
        "        self.final_answer: bool = False\n",
        "\n",
        "    def on_llm_new_token(self, token: str, **kwargs: any) -> None:\n",
        "        self.content += token\n",
        "        if \"Final Answer\" in self.content:\n",
        "            # now we're in the final answer section, but don't print yet\n",
        "            self.final_answer = True\n",
        "            self.content = \"\"\n",
        "        if self.final_answer:\n",
        "            if '\"action_input\": \"' in self.content:\n",
        "                if token not in [\"}\"]:\n",
        "                    sys.stdout.write(token)  # equal to `print(token, end=\"\")`\n",
        "                    sys.stdout.flush()\n",
        "\n",
        "agent.agent.llm_chain.llm.callbacks = [CallbackHandler()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCoyjqoVOyCN"
      },
      "source": [
        "Let's try again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Sc2vKO1OyCN"
      },
      "outputs": [],
      "source": [
        "agent(\"what is the square root of 71?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZWkDS4BOyCN"
      },
      "outputs": [],
      "source": [
        "agent.agent.llm_chain.llm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbgegL_nOyCN"
      },
      "source": [
        "It isn't perfect, but this is getting better. Now, in most scenarios we're unlikely to simply be printing output to a terminal or notebook. When we want to do something more complex like stream this data through another API, we need to do things differently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zl9psp-vOyCN"
      },
      "source": [
        "## Using FastAPI with Agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNKWpUS2OyCN"
      },
      "source": [
        "In most cases we'll be placing our LLMs, Agents, etc behind something like an API. Let's add that into the mix and see how we can implement streaming for agents with FastAPI.\n",
        "\n",
        "First, we'll create a simple `main.py` script to contain our FastAPI logic. You can find it in the same GitHub repo location as this notebook ([here's a link](https://github.com/pinecone-io/examples/blob/langchain-streaming/learn/generation/langchain/handbook/09-langchain-streaming/main.py)).\n",
        "\n",
        "To run the API, navigate to the directory and run `uvicorn main:app --reload`. Once complete, you can confirm it is running by looking for the ðŸ¤™ status in the next cell output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzkLd0YjOyCN",
        "outputId": "a2b780c3-b598-4e4b-9772-188200ff9419"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'status': 'ðŸ¤™'}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "res = requests.get(\"http://localhost:8000/health\")\n",
        "res.json()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rf679EEtOyCO",
        "outputId": "6c38bc13-4eb5-4792-c182-af9e98e6a5a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Response [200]>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res = requests.get(\"http://localhost:8000/chat\",\n",
        "    json={\"text\": \"hello there!\"}\n",
        ")\n",
        "res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hu37nxROOyCO",
        "outputId": "1bc1450b-7793-4246-c1b3-403273b230d8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input': 'hello there!',\n",
              " 'chat_history': [],\n",
              " 'output': 'Hello! How can I assist you today?'}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res.json()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjfaqDneOyCO"
      },
      "source": [
        "Unlike with our StdOut streaming, we now need to send our tokens to a generator function that feeds those tokens to FastAPI via a `StreamingResponse` object. To handle this we need to use async code, otherwise our generator will not begin emitting anything until _after_ generation is already complete.\n",
        "\n",
        "The `Queue` is accessed by our callback handler, as as each token is generated, it puts the token into the queue. Our generator function asyncronously checks for new tokens being added to the queue. As soon as the generator sees a token has been added, it gets the token and yields it to our `StreamingResponse`.\n",
        "\n",
        "To see it in action, we'll define a stream requests function called `get_stream`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PC-h9Gi6OyCP"
      },
      "outputs": [],
      "source": [
        "def get_stream(query: str):\n",
        "    s = requests.Session()\n",
        "    with s.get(\n",
        "        \"http://localhost:8000/chat\",\n",
        "        stream=True,\n",
        "        json={\"text\": query}\n",
        "    ) as r:\n",
        "        for line in r.iter_content():\n",
        "            print(line.decode(\"utf-8\"), end=\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LP3irJFYOyCP",
        "outputId": "313c77e4-0470-439f-e8b3-40cadc247d1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \"Hello! How can I assist you today?\"\n"
          ]
        }
      ],
      "source": [
        "get_stream(\"hi there!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yoN-Fl3OyCP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}