{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sensationalspace/colab/blob/main/MusicGen_Gradio_Demo_v1_2_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MusicGen Gradio Demo\n",
        "\n",
        "Execute the first cell to install, the second one to run the demo.\n",
        "\n",
        "More info on our repository [facebookresearch/audiocraft](https://github.com/facebookresearch/audiocraft/) or our [research paper](https://arxiv.org/abs/2306.05284).\n",
        "\n",
        "First eval with a model will be slower because of the model download. Melody model should take 30 seconds for 15 seconds of audio."
      ],
      "metadata": {
        "id": "efhHAvBOgMnL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Execute first for installing AudioCraft\n",
        "# @markdown Execute this cell to install AudioCraft. (**Note:** the `pip install` stage might take a while.)\n",
        "from pathlib import Path\n",
        "import subprocess as sp\n",
        "from shutil import rmtree\n",
        "import re\n",
        "import time\n",
        "\n",
        "def run_log_on_fail(name, *args, **kwargs):\n",
        "    log = log_folder / (name + \".log\")\n",
        "    print(\"Running stage\", name)\n",
        "    try:\n",
        "        sp.run(*args, **kwargs, stdout=open(log, 'w'), stderr=sp.STDOUT, check=True)\n",
        "    except sp.CalledProcessError:\n",
        "        print(f\"Stage {name} failed, this is NOT expected, see logs hereafter.\")\n",
        "        print(open(log, 'r').read())\n",
        "        raise\n",
        "    else:\n",
        "        return log\n",
        "\n",
        "def install():\n",
        "    global log_folder\n",
        "    ac_folder = Path('/content/audiocraft')\n",
        "    log_folder = Path('/content/logs')\n",
        "    log_folder.mkdir(exist_ok=True, parents=True)\n",
        "    if ac_folder.exists():\n",
        "        rmtree(ac_folder)\n",
        "    try:\n",
        "        run_log_on_fail(\"git clone\", [\"git\", \"clone\", \"https://github.com/facebookresearch/audiocraft\"], cwd=ac_folder.parent)\n",
        "        run_log_on_fail(\"pip install\", [\"pip\", \"install\", \"-e\", \".\"], cwd=ac_folder)\n",
        "        pass\n",
        "    except sp.CalledProcessError:\n",
        "        print(\"INSTALLATION FAILED...\")\n",
        "        return\n",
        "\n",
        "install()"
      ],
      "metadata": {
        "id": "NJXpj7lyYjve",
        "outputId": "fee26581-ac37-4a0c-8145-f7049feeb628",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running stage git clone\n",
            "Running stage pip install\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Execute then to launch the demo\n",
        "#@markdown Wait until you see `Running on public URL: ` in the link with a *.gradio.live url. Click on the URL, and enjoy!\n",
        "!cd /content/audiocraft && python -m demos.musicgen_app --share"
      ],
      "metadata": {
        "id": "0mFH0Nma65RU",
        "outputId": "f4433205-7684-402c-8e8e-dad3d271ef84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
            "    PyTorch 2.1.0+cu121 with CUDA 1201 (you have 2.1.0+cu118)\n",
            "    Python  3.10.13 (you have 3.10.12)\n",
            "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
            "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
            "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
            "2023-11-08 21:05:45.615920: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-08 21:05:45.615989: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-08 21:05:45.616034: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-08 21:05:45.624554: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-08 21:05:46.837400: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-11-08 21:05:47.903460: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-08 21:05:47.904119: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-08 21:05:47.904291: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "False\n",
            "Running on local URL:  http://127.0.0.1:7860\n",
            "Running on public URL: https://e0bc7730319f5bffbf.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "Loading model facebook/musicgen-stereo-melody\n",
            "Downloading state_dict.bin: 100% 2.82G/2.82G [00:39<00:00, 70.6MB/s]\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/demucs/hybrid_transformer/955717e8-8726e21a.th\" to /root/.cache/torch/hub/checkpoints/955717e8-8726e21a.th\n",
            "100% 80.2M/80.2M [00:01<00:00, 54.9MB/s]\n",
            "INFO:audiocraft.modules.conditioners:T5 will be evaluated with autocast as float32\n",
            "Downloading (…)ve/main/spiece.model: 100% 792k/792k [00:00<00:00, 12.6MB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 1.39M/1.39M [00:00<00:00, 42.6MB/s]\n",
            "Downloading (…)lve/main/config.json: 100% 1.21k/1.21k [00:00<00:00, 6.91MB/s]\n",
            "Downloading model.safetensors: 100% 892M/892M [00:07<00:00, 118MB/s]\n",
            "Downloading (…)ssion_state_dict.bin: 100% 589/589 [00:00<00:00, 2.38MB/s]\n",
            "INFO:root:Getting pretrained compression model from HF facebook/encodec_32khz\n",
            "Downloading (…)lve/main/config.json: 100% 758/758 [00:00<00:00, 3.05MB/s]\n",
            "Downloading model.safetensors: 100% 236M/236M [00:01<00:00, 187MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
            "new batch 1 ['afrobeat summer feel'] [None]\n",
            "INFO:audiocraft.modules.codebooks_patterns:New pattern, time steps: 500, sequence steps: 504\n",
            "Make a video took 4.6311352252960205\n",
            "batch finished 1 31.88257884979248\n",
            "Tempfiles currently stored:  2\n",
            "Loading model facebook/musicgen-stereo-melody\n",
            "new batch 1 ['afrobeat summer feel'] [None]\n",
            "INFO:audiocraft.modules.codebooks_patterns:New pattern, time steps: 1500, sequence steps: 1504\n",
            "INFO:audiocraft.modules.codebooks_patterns:New pattern, time steps: 1200, sequence steps: 1204\n",
            "Make a video took 17.031403303146362\n",
            "batch finished 1 159.23580479621887\n",
            "Tempfiles currently stored:  4\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://e0bc7730319f5bffbf.gradio.live\n",
            "Process ForkProcess-3:\n",
            "Process ForkProcess-4:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
            "    call_item = call_queue.get(block=True)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
            "    call_item = call_queue.get(block=True)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 103, in get\n",
            "    res = self._recv_bytes()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2t1qfoTZSdSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CxIloF2d69kJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}